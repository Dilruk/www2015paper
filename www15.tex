% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

%\documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}

\usepackage[colorlinks=true,linkcolor=blue]{hyperref}	% For hyper links
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bbm}

\usepackage{CJKutf8} 
\usepackage{ucs} 
\usepackage[encapsulated]{CJK} 
\newcommand{\myfont}{gbsn}
\usepackage[usenames]{color}
\newcommand{\red}[1]{\textcolor{red}{#1}}

%\usepackage{fontspec} 
%\usepackage{xeCJK}
%\setCJKmainfont{BiauKai}

%\usepackage{polyglossia}
%\setmainlanguage{english}
%\setotherlanguage{arabic}
%\newfontfamily\arabicfont{Scheherazade}

\graphicspath{ {./Figures/} }

\begin{document}

\title{A Seedless Method for Identifying Interesting Moments in Social Media Streams}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Cody Buntain\\
       \affaddr{Dept. of Computer Science}\\
       \affaddr{University of Maryland}\\
       \affaddr{College Park, Maryland 20742}\\
       \email{cbuntain@cs.umd.edu}
% 2nd. author
\alignauthor
Jimmy Lin\\
       \affaddr{College of Information Studies}\\
       \affaddr{University of Maryland}\\
       \affaddr{College Park, Maryland 20742}\\
       \email{jimmylin@cs.umd.edu}
\alignauthor
Jen Golbeck\\
       \affaddr{College of Information Studies}\\
       \affaddr{University of Maryland}\\
       \affaddr{College Park, Maryland 20742}\\
       \email{golbeck@cs.umd.edu}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.
}

\maketitle
\begin{abstract}
Existing event detection research in social media often targets specific events by tracking human-generated seed keywords related to the event.
While capable, such methods suffer from significant drawbacks, the most important of which is the inability to discover related, interesting, and unexpected moments that may not specifically mention any predefined keyword. 
This paper extends these approaches by introducing a general technique for identifying compelling moments without seed keywords by modeling temporal patterns around bursts and by building a classifier that can identify bursting tokens from Twitter's public sample stream.
Since many events are also interesting moments, we are also able to subsume some event detection tasks and achieve a measure of language agnosticism and general flexibility current techniques lack.
To demonstrate our approach's potential, we compare a baseline event-detection system with our language-agnostic algorithm in detecting a number of key events across three major sporting competitions: the 2013 World Series, 2014 Super Bowl, and 2014 World Cup.
Results from this comparison show our method demonstrates similar performance in this sports detection domain without the need to pre-specify any event-related information.
We then go further by applying our sports-based models to the task of identifying earthquakes in Japan and show our method detects large spikes in earthquake-related tokens within two minutes of the actual event.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.2.8}{Database Applications}{Data Mining}
\category{\\H.3.3}{Information Search and Retrieval}{Information Filtering}

\keywords{event detection, twitter, social networks, temporal features}

\section{Introduction}

Social media's shear volume (hundreds of thousands of comments, statuses, and photos generated per minute on Facebook alone as of 2012 \cite{Pring2012}) has researchers, traditional media organizations, and even advertisers considering it as a supplement to existing sources of breaking and real-time news.
This push has led to a proliferation of socially enabled systems for detecting noteworthy events across a wide field of domains.
Such systems generally require a human expert to generate domain-specific seed tokens to track across social networks, and while these systems are able to detect noteworthy events rapidly, they often come with restrictions that make the event-detection task tractable. 
These constraints often include predefined classes of interesting events, restrictions on target languages, and the need to filter data through expensive language models.
Resulting systems are brittle and inflexible in application and are difficult to adapt to new domains, languages, or types of events.
Most importantly though, the critical weakness is that such approaches preclude any notion of \emph{discovering} interesting events; since little flexibility exists in identifying new tokens automatically, many interesting moments related to the target event are lost because they may not mention a pre-specified token.
Addressing this significant drawback is difficult given the volume of data and noise in social media streams, but the flexibility to discover surprising or unexpected moments across multiple languages, dialects, and domains has many practical advantages currently unsatisfied by existing technology.

To explore these questions, we propose a general method for learning temporal patterns of increased token usage, or bursts, and tracking the frequency of tokens experiencing these bursts to identify interesting moments in social media streams.
In contrast to existing work, our approach operates without prior knowledge of the target events or any domain-dependent stream filtering.
We demonstrate this additional flexibility through experiments on Twitter's public sample stream surrounding key moments in large sporting competitions
Sporting events are both highly followed and occur regularly, which make them ideal for collecting data and exercising event detection systems but are also complex in that they include multiple types of and unpredictable patterns of events around scores, fouls, and other compelling moments of action.
High-impact sports competitions like the World Cup also have extensive international viewership, so one must also account for multiple languages and different vocabularies.
Given the overlap in interesting moments and events, we compare our language-agnostic, burst-centric technique to a baseline keyword-centric method for detecting events in several such sporting competitions.
Finally, we transfer the models learned in the sporting domain to earthquake detection and show performance commensurate with existing socially enabled earthquake detection research.

More concisely, this work makes the following contributions:
%
\begin{itemize}
\item We propose a feature set and classifier for seedless, language-agnostic detection of moments of high interest in the Twitter public sample stream.
\item By comparing our method to an existing event detection baseline technique, we demonstrate our generalization of events to interesting moments is competitive with existing approaches.
\item It is possible to adapt our sports-related models to other domains like earthquake detection and still maintain comparable levels of performance to domain-specific systems.
\end{itemize}

\section{Related Work}
\label{sect:relatedWork}

Though we are focusing on the slightly different problem of identifying ``interesting moments'' in social media streams, we still build on much of the same work as classical event detection, which has fascinated researchers for over twenty years.
This subfield has evolved to integrate the latest available techniques and data sources, starting from early digital newsprint to blogs and now social media.
Early stages of this research began in the mid-nineties with the Topic Detection and Tracking (TDT) initiative.
These programs demonstrated feasibility in detecting new topics from traditional media sources, but as Allan, Papka, and Lavrenko discussed in 1998, these approaches required additional work to see real success \cite{allan1998line}.
Even in that early work, Allan et al. were already discussing the tradeoffs of using pre-defined keyword sets and event classes when detecting new events, which is still a struggle for researchers today.
%It is also important to note that, at this nascent stage, topic detection and event detection were relatively synonymous.

Though this early research focused primarily on topic detection from newsprint and traditional media sources, Kleinberg's 2002 paper altered the landscape by applying topic detection to non-traditional data sources like his personal email archive and by introducing what appears to be the first real treatment of burst analysis in ``document streams that arrive continuously over time'' rather than  static collections \cite{Kleinberg:2002:BHS:775047.775061}. 
Despite introducing the streaming context, Kleinberg cast topic detection as a retrospective, state-based optimization problem.
He then leveraged existing work on hidden Markov models (HMMs) to find sequences of high usage keywords, or bursts, from which he could detect events (and construct complex nested states to develop event hierarchies).
Kleinberg's examination here laid much of the foundation for the focus on topic bursts that characterized much of the proceeding research in this area.

Following from Kleinberg's work and the increasing size of digital content on the Internet, several new approaches to topic detection emerged.
Notably, topic detection divided into two distinct tasks: identifying topics in data via algorithms like Latent Dirichlet Allocation (LDA) \cite{blei2003latent} and detecting events from text.
Our research focuses on events rather than topics, so early event detection work like that from Fung et al. in 2005 is especially interesting \cite{Fung:2005:PFB:1083592.1083616}.
Fung et al. extended Kleinberg's burst detection scheme by identifying bursty keywords from digital newspapers and clustering these keywords into groups to identify bursty events, which displayed success in identifying trending events in an English-language newspaper from Hong Kong.

%Along with new interest in burst-centric research, another important trend starting around this time was the integration of additional data sources beyond traditional newsprint, specifically blog content.
%Blogs offered direct insight into the social consciousness in a way that was previously unavailable via traditional media since blogs include a great deal of social information regarding the author.
%Zhao et al. took advantage of this additional social information in their 2007 work on flow-based event detection from 2007 \cite{Zhao2007}.
%By integrating this social data with the textual and temporal techniques described above, Zhao et al. were able to identify events with high accuracy in two social datasets: the Enron email dataset and the Dailykos dataset.
%Similarly, Bansal's group from the University of Toronto developed the Blogscope project to identify trending and bursty keywords by location as well as time across the entire ``blogosphere'' \cite{Bansal:2007:BSO:1325851.1326028,Bansal:2007:BSA:1242572.1242802}.

%\subsection{Event Detection in Microblogs}

Soon after, the research community began experimenting with alternative media sources like blogs, but real gains came when microblogging platforms began their rise in popularity.
These microblogging platforms include Twitter and Sina Weibo and are characterized by constrained post sizes (e.g., Twitter constrains user posts to 140 characters) and broadcasting publicly consumable information.
Since their rise, a great deal of research has explored how data posted to these networks can be leveraged for social good projects like event detection.
Much of this work, however, is retrospective in nature and focuses on detecting events after the fact rather than utilizing one of the real advantages in these social networks: unlike newspapers and blogs, these networks produce huge volumes of information that can be processed in \emph{real time}.

One of the most well-known works in detecting events from microblog streams is Sakaki, Okazaki, and Matsuo's 2010 paper on detecting earthquakes in Japan using Twitter \cite{Sakaki:2010:EST:1772690.1772777}.
Sakaki et al. show that not only can one detect earthquakes on Twitter but also that it can be done simply by tracking frequencies of earthquake-related tokens.
Surprisingly, this approach can outperform geological earthquake detection tools since digital data propagates faster than tremor waves in the Earth's crust.
Though this research is limited in that it requires pre-specified tokens and is highly domain- and location-specific (Japan has a high density of Twitter users, so earthquake detection may perform less well in areas with fewer Twitter users), it demonstrates a significant use case and the potential of such applications.

Along with Sakaki et al., 2010 saw two other relevant papers: Lin et al.'s construction of a probabilistic popular event tracker \cite{Lin:2010:PSM:1835804.1835922} and Petrovi\'{c}, Osborne, and Lavrenko's application of locality-sensitive hashing (LSH) for detecting first-story tweets from Twitter streams \cite{Petrovic:2010:SFS:1857999.1858020}.
Lin's work is interesting for a number of reasons: first, it circumvents the need for language model-based stop word lists by using probabilistic models to discriminate between common and informational tokens.
Secondly, Lin's integration of social and structural features into the event detection task demonstrated that real performance enhancements can be gained through non-textual features.
Thirdly, their paper relates well to Kleinberg's initial work on bursty topic detection by illustrating how his state machine approach is a degenerate case of the PET model.
Like the majority of its contemporary systems, however, PET also requires seeding with a pre-specified list of tokens to guide its event detection and concentrates on retrospective per-day topics and events.

In contrast, Petrovi\'{c} and his colleagues' clustering research in Twitter avoids the need for seed keywords and retrospective analysis by instead focusing on the practical considerations of clustering large streams of data quickly.
That is, rather than construct a probabilistic mixture model for each token, Petrovi\'{c} focuses on methods for clustering tweets that contain similar tokens into topical clusters.
While typical clustering algorithms require distance calculations for all pairwise messages, LSH facilitates rapid clustering at the scale necessary to support event detection in Twitter streams by restricting the number of tweets compared to only those within some threshold of similarity.
Once these clusters are generated, Petrovi\'{c} was able to track their growth over time to determine impact for a given event.
This research was originally unique in that it was one of the early methods that did not require pre-specified seed tokens for detecting events and has been very influential in the field, resulting in a number of additional publications to demonstrate its utility in breaking news and for high-impact crisis events \cite{osborne2014real,petrovic2013can,6601695}.
That being said, a key weakness in Petrovi\'{c}'s work is its reliance on semantic similarity between tweets, which limits its ability to operate in mixed-language environments.

Similar to Petrovi\'{c}, Weng and Lee's 2011 paper on EDCoW, short for Event Detection with Clustering of Wavelet-based Signals, is also able to identify events from Twitter without seed keywords \cite{weng2011event}.
After stringent filtering (removing stop words, common words, and non-English tokens), EDCoW uses wavelet analysis to isolate and identify bursts in token usage as a sliding window advances along the social media stream.
Highly significant bursts are then cast as a cross-correlation matrix against which a graph partitioning algorithm is run to construct topical clusters from bursty tokens.
Besides the heavy filtering of the input data, this approach exhibits notable similarities with the language-agnostic method we describe herein with its reliance on bursts to detect event-related tokens; the methods described in Weng and Lee's paper, however, operate in a more retrospective manner, focusing on the level of daily news rather than breaking event detection on which our research focuses.
Becker, Naaman, and Gravano's 2011 paper on identifying events in Twitter also fall under this label of retrospective analysis, but their findings also demonstrate reasonable performance in identifying events in Twitter by leveraging classification tasks to separate tweets into those on ``real-world events'' versus non-event messages \cite{becker2011beyond-tr,becker2011beyond}.
Similarly, Diao et al. also employ a retrospective technique to separate tweets into global, event-related topics and personal topics \cite{diao2012finding}.

Beyond these works, several domain-specific research efforts have targeted sporting events specifically\cite{vasudevan2013twitter,Zhao2011,lanagan2011using}.
Lanagan and Smeaton's work is of particular interest because it relies almost solely on Twitter's per-second message volume \cite{lanagan2011using}.
Though naive, this frequency approach is able to detect large bursts on Twitter in high-impact events  without reliance on complex linguist analysis and performs well in streaming contexts as little information must be kept in memory.
Unfortunately, two main disadvantages exist with this technique: first, detecting a burst would provide evidence of an event, but it would be difficult to gain insight into what that event actually was without additional processing.
Secondly, as described in their paper, a pure volumetric approach is hampered by limitations on Twitter's public stream, which has an upper limit on the number of messages per minute one can capture.

Finally, the most recent work relevant to our research is the 2013 paper by Xie et al. on TopicSketch \cite{xie2013topicsketch}.
Like us, TopicSketch's authors seek to perform real-time event detection from Twitter streams ``without pre-defined topical keywords'' by maintaining acceleration features across three levels of granularity: individual token, bigram, and total stream.
As with Petrovi\'{c}'s use of LSH, Xie et al. leverage ``sketches'' and dimensionality reduction to facilitate event detection and also relies on language-specific similarities.
Furthermore, Xie et al. focus only on tweets from Singapore rather than the worldwide stream.
In contrast, our approach is differentiated primarily in its language-agnosticism and its use of the unfiltered stream from Twitter's global network.

Despite this extensive body of research, it is worth asking how event detection on Twitter streams differs from Twitter's own offerings on ``Trending Topics,'' which they make available to all their users.
When a user visit's Twitter's website, she is immediately greeted with her personal feed as well as a listing of trending topics for her city, country, worldwide, or nearly any location she chooses.
These topics offer insight into the current popular topics on Twitter, but the main differentiating factor is that these popular topics are not necessarily connected to specific events.
Rather, popular memetic content like ``\#MyLovelifeInMoveTitles'' often appear on the list of trending topics.
Additionally, Twitter monetizes these trending topics as a form of advertising \cite{Sydell2011}.  
These trending topics also can be more high-level than the point events that we seek to identify: for instance, during the World Cup, particular matches or the tournament in general were identified as trending topics by Twitter, but individual events like goals or penalty cards in those matches were not.
It should be clear then that Twitter's trending topics serves a different purpose than the streaming event detection described herein.

\section{Techniques for Streaming Event Detection}
\label{sect:methods}

This paper's primary goal is to demonstrate the feasibility of detecting highly interesting moments from social media streams by identifying bursting tokens and analyzing their frequencies (we refer to this language-agnostic, burst-centric technique as LABurst).
Since compelling moments often align with significant events, we compare LABurst's performance with that of a baseline seed-token technique in detecting specific events within a variety of major sporting competitions.
This comparison shows our more general approach can still perform on par with existing domain-specific methods.
Then, we show applications outside the sports domain by applying LABurst to the task of detecting earthquakes using social media similar to Sakaki et al.
This section defines these detection tasks, presents our experimental framework, details the data sources used, and outlines both the baseline technique and our LABurst method.

\subsection{Problem Definition}

%Prior to any deep description on methods, we first define an ``event'' and a ``burst'' within the context of this research.
%Though the term ``event'' is vague and overridden in many contexts, we borrow from existing works like Allan et al. and Raimond and Abdallah's Event Ontology \cite{Raimond2007,allan1998line}.
%From these sources, one could define an event as ``something that happens at a particular time and place'' and can be composed of one or more ``sub-events'' that occur within the context of the larger event.
%For the purposes of this paper, however, we further restrict our definition to \emph{instantaneous} events and sub-events, or those events that occur at a specific place and time \emph{and} have a negligible amount of time between start and end.
%Examples of such instantaneous events include a point scored in a game, a single explosion during some terrorist attack, or a particular tremor in an earthquake.
%These instantaneous events are interesting and hard to identify because one may not know tokens relevant to these brief events until the event is occurring, and by the time one can react, such a short-term event would already be over. 
%
The term ``event'' is vague and overridden in many contexts, with varying definitions from existing works like Allan et al. to an entire ontology devoted to describing the term (Raimond and Abdallah's Event Ontology) \cite{Raimond2007,allan1998line}.
By instead focusing on interesting moments in time, we avoid this myriad of definitions and restrict ourselves to detecting \emph{instantaneous} moments of interest in the real world.
Examples of such moments include a point scored in a game, a single explosion during a particular terrorist attack, or a discrete tremor in an earthquake.
These instances are interesting and hard to identify because one may not know tokens relevant to these brief events until the event is occurring, and by the time one can react, the moment has passed. 

Corresponding to these brief moments, we define a burst in a token's usage as a sudden increase in the frequency with which that token appears in a data stream, and we refer to such tokens as ``bursty.''
These bursts need not be symmetric in that a drastic uptick in frequency need not be followed by an equally dramatic fall in usage.

If we let $E$ denote the set of all minutes $t$ in which an event occurs, then the indicator function $\mathbbm{1}_E(t)$ returns a $1$ for all times $t$ in which an event occurs, and $0$ for all other values of $t$. 
We can now define the event detection task as constructing a function that approximates this indicator function $\mathbbm{1}_E(t)$.
To account for possible lag in experiencing the event, typing out a message about the event, and the message actually posting to a social media server, we include a parameter $\tau$ to control for tolerance of delay.
This parameter relaxes the task slightly by constructing the set $E'$ where, for all $t \in E$, $t, t+1, t+2, ..., t+\tau \in E'$.
Since our evaluation is a comparison between two methods that share the same ground truth, and controlling $\tau$ affects the ground truth consistently for both methods, comparative results should be unaffected.
In this paper, we use $\tau=2$.

False positives/negatives and true positives/negatives follow in the normal way for some candidate function $\widehat{\mathbbm{1}_{E'}}(t)$: a false positive is any time $t$ such that $\widehat{\mathbbm{1}_{E'}}(t) = 1$ and ${\mathbbm{1}_{E'}}(t) = 0$; likewise, a false negative is any $t$ such that $\widehat{\mathbbm{1}_{E'}}(t) = 0$ and ${\mathbbm{1}_{E'}}(t) = 1$.
True positives/negatives then follow as expected.

\subsection{Experimental Framework}

\red{So my issue with this section is that it combines the evaluation framework, e.g., how you do ROC computation, with the data collection and ground truth determination. Here you really just want to say, the input to the ground truth is a vector of indicator variables (for each time slice), from which you construct an ROC curve. Leave the description of the data gathering to a late section. My suggested reorg would be Section (3): problem def and eval frameworkl Section (4) baseline, Section (5) LABurst, Section (6) Experimental Setup}


Having established the vocabulary and tasks, we can now turn our attention to the actual mechanics of comparing our LABurst technique with the baseline.
Sporting competitions are full of interesting and unpredictable moments, such as scores, fouls, ejections, or other dramatic instances of play.
Additionally, sporting events with large followings occur fairly often and with a good degree of regularity, greatly simplifying data collection.

Our main experiment is a comparative study in detecting a collection of events in three major sporting competitions: the 2013 Major League Baseball (MLB) World Series, 2014 National Football League (NFL) Super Bowl, and 2014 F\'{e}d\'{e}ration Internationale de Football Association (FIFA) World Cup.
Specifically, for the 2013 World Series between the Boston Red Sox and the St. Louis Cardinals, we explore the final two games on 28 October and 30 October of 2013.
Similarly for the 2014 World Cup, our analysis covers the final two matches of tournament: the 12 July match between the Netherlands and Brazil for third place, and the final match on 13 July between Germany and Argentina for first place.
Since the 2014 Super Bowl was a single-day competition, we covered the entire game between the Seattle Seahawks and the Denver Broncos on 2 February 2014.

In each competition, we extract the times of four basic types of events: beginning of the game, end of the game, scores, and penalties.
We capture these events and times from sport journalism articles, game highlights, box scores, blog posts, and social media messages.
Table \ref{tab:eventStats} provides some statistics on the events we identified.
%
\begin{table}[htdp]
\caption{Sporting Competition Event Counts}
\begin{center}
\begin{tabular}{|p{0.7in}|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Sport}} & \textbf{Start} & \textbf{End} & \textbf{Score} & \textbf{Penalty} & \textbf{Sum} \\ \hline
World Series & 2 & 2 & 10 & 0 & 14 \\ \hline
Super Bowl & 1 & 1 & 10 & 0 & 12 \\ \hline
World Cup & 2 & 2 & 4 & 9 & 17 \\ \hline
\multicolumn{5}{|r|}{\textbf{Total}} & 43 \\ \hline
\end{tabular}
\end{center}
\label{tab:eventStats}
\end{table}

\red{So you've described the test data, right? What about the training data?}

These events then comprise the ground truth against which we compare LABurst and the baseline.
That is, positive instances of events are those minutes in which one of these events occurs, and negative instances are those minutes in which no known sports-related event occurs.
This data was also excluded from development such that neither the baseline nor LABurst are trained on or evaluated against this data prior to experimentation.
From this data, we generate receiver operating characteristic (ROC) curves for both LABurst and the baseline across each sport and then construct a composite ROC curve for all sports.
We then evaluate relative performance between the two approaches by comparing the area under the curves (AUCs) for both methods.
To form these ROC curves, varied threshold parameters for each method.
In the baseline approach, our threshold parameter controls the minimum difference between current frequency and average frequency over the sliding window.
For our LABurst method, the ROC curve is generated by varying the minimum number of tweets a window must contain for an event to be detected.
The AUC of the ROC curve is useful in this instance because it is robust against imbalanced classes, which we expect to see in the event detection task.

We then follow up with an additional experiment of whether LABurst can detect earthquake events as inspired by Sakaki et al.
This investigation compares LABurst with the frequency of the keyword ``earthquake'' by applying LABurst's models learned in the first experiment to social media captured during two large earthquakes: the 7.1-magnitude quake off the coast of Honshu, Japan on 25 October 2013, and a 6.5-magnitude quake off the coast of Iwaki, Japan on 11 July 2014.

\subsubsection{Data Collection and Additional Events}

%Though this work should apply to any social media stream, much of the development leverages Twitter since a large amount of research on this social network already exists, and data is relatively easy to acquire.
Our data on the main sporting events used for evaluation (the World Series, Super Bowl, and World Cup) come from Twitter's 1\% public sample stream using code from Jimmy Lin's twitter-tools library\footnote{https://github.com/lintool/twitter-tools}.
For training LABurst, however, we require additional data to model bursty events, so we leverage two other Twitter data sources as well: an excerpt from the Edinburgh Twitter Corpus \cite{Petrovic:2010:ETC:1860667.1860680} and a selection of tweets from the Twitter Firehose covering Argentina in November of 2011.
From these sources, we are able to generate a series of timestamped event timelines and related tokens for several additional sporting events as well: %
%
\begin{itemize} 
\item The 2010 NFL National Football Championship game, 
\item Four premier league soccer games in November of 2012, %(Arsenal de Sarand\`{i} versus V\'{e}lez Sarsfield on November 3, Barcelona versus Mallorca on November 11, Argentina versus Saudi Arabia on November 14, and Argentina versus Brazil on November 22), 
\item The National Hockey League's (NHL) 2014 playoffs, 
\item The National Basketball Assoc. (NBA) 2014 playoffs, 
\item The 2014 Kentucky Derby and Belmont Stakes races, 
\item And six early days in the 2014 FIFA World Cup.
\end{itemize}

\subsection{Baseline Event Detection}

Inspired by Twitter's blog post on detecting goals during the World Cup, we constructed a simple detector that uses per-minute frequencies for a small set of seed tokens \cite{Cipriani2014}. 
These seed tokens are likely to exhibit bursts in usage during events, such as ``goal'' for goals in soccer/football or hockey or ``run'' for runs scored in baseball.
Our baseline implementation also includes some rudimentary normalization to collapse modified words to their originals (e.g., ``gooaallll'' down to ``goal'').
We know such a technique is effective because, as seen in the related work, many existing stream-based event detection systems use just such an approach to track specific types of events.

At a high level, we compare the most recent count of these target tokens against the average count over the past few minutes, and if the difference is above some threshold, we claim an event occurred.
Formally, we define the following: a time series $T$ segmented into $m$ minutes, a set of event-related seed tokens $S$ such that $s_i \in S$ is one of these event-related tokens, and a function \texttt{count($s_i, t_j$)} that returns the frequency of token $s_i$ in minute $t_j$.
The frequency for a given minute $t_j$ is then defined by the function \texttt{freq($t_j$)} shown in Eq. \ref{eq:minuteFrequency}.
We also construct a sliding window $w$ of size $|w|$ such that $w_k$ includes minutes $t_k$ to $t_{k+|w|-1}$ and define an average over this window as \texttt{avg($w_k$)}, shown in Eq. \ref{eq:windowAverage}.
%
\begin{equation}
\label{eq:minuteFrequency}
\text{freq}(t_j) = \sum_{i=0}^{|S|}\text{count}(s_i, t_j)
\end{equation}
\begin{equation}
\label{eq:windowAverage}
\text{avg}(w_k) = \frac{\sum_{j=k}^{k + |w| - 1}\text{freq}(t_j)}{|w|}
\end{equation}
%
Given these functions, we take the difference $\Delta_k$ between the frequency at time $t_{k+|w|-1}$ and the average for window $w_k$ such that $\Delta_k = \;$\texttt{freq}$(t_{k+|w|-1}) - $ \texttt{avg($w_k$)}.
If this difference exceeds some threshold $Z$ such that $\Delta_k > Z$, we say an event was detected at time $t_{k+|w|-1}$.

Since our analysis covers three separate types of sporting competitions, the seed keyword list for this method should include tokens from the vocabulary of each. 
We avoid separate keyword lists for each sport to provide an even comparison to the general nature of our language-agnostic technique.
The keywords for which we searched are shown in Table \ref{tab:targetTokens}, and we took the union of these token sets.
Additionally, the following regular expressions collapsed deliberately misspelled tokens down to their normal counterparts: ``g+o+a+l+'' $\rightarrow$``goal'', ``g+o+l+'' $\rightarrow$``gol'', ``g+o+l+a+z+o+'' $\rightarrow$``golazo'', ``sco+red?'' $\rightarrow$``score''.

%
\begin{table}[htdp]
\caption{Predefined Seed Tokens}
\begin{center}
\begin{tabular}{|p{0.7in}|p{2.0in}|}
\hline
\multicolumn{1}{|c|}{\textbf{Sport}} & \multicolumn{1}{|c|}{\textbf{Tokens}} \\ \hline
World Series & ``run'', ``home'', ``homerun'' \\ \hline
Super Bowl & ``score'', ``touchdown'', ``td'', ``fieldgoal'', ``points'' \\ \hline
World Cup & ``goal'', ``gol'', ``golazo'', ``score'', ``foul'', ``penalty'', ``card'', ``red'', ``yellow'', ``points'' \\ \hline
\end{tabular}
\end{center}
\label{tab:targetTokens}
\end{table}

\subsection{Language-Agnostic Event Detection}

As mentioned, existing research achieves acceptable performance by tracking frequencies for a small set of bursty keywords when a real-world event occurs.
\red{Make clear here that the seed list IS NOT an input to your algorithm... (right?)}
We extend such approaches by automatically identifying these bursty tokens and detecting interesting moments based on those frequencies.
To this end, we constructed a set of temporal features to model the characteristics of a bursty token.
Like other approaches, these temporal features were built around a sliding window of several minutes with each window further divided into overlapping slices. %as shown in Figure \ref{fig:windowSlices}.
In this manner, we construct a windowed time series containing the most recent frequencies for each token, from which our features are generated.

%\begin{figure}[hbtp]
%\begin{center}
%\includegraphics[width=1.75in]{./figures/slidingWindowSlices.pdf}
%\caption{Sliding Window and Overlapping Slices}
%\label{fig:windowSlices}
%\end{center}
%\end{figure}

From these features, we use AdaBoost to combine support vector machines (SVMs) and random forests (RFs) into an ensemble classifier to discriminate between bursty and non-bursty tokens.
To cast this task as a machine learning problem, however, we require positive and negative token samples on which our classifier can be trained.
While obtaining positive samples of bursty tokens is straightforward (we can at least use keywords from the baseline as well as event-specific tokens like names of scoring players or locations of events), determining negative samples is difficult since one cannot know all events that may burst on Twitter at a given moment.
We circumvent this difficulty by automatically classifying all stop words as negative, non-bursty tokens, and Python's NLTK\footnote{http://www.nltk.org} library provides convenient lists of stop words for several languages.

It is important to note that we perform this classification without any semantic or language-based filtering or normalization, and the only data we discard in this analysis are retweets and hashtags.

\subsubsection{Temporal Features}

While previous work already covers burst detection, only a few are extensible to the streaming case, and we integrate those into our system.
We also develop a number of features that should yield higher weights for tokens that deviate significantly from normal posting frequencies:

\begin{itemize}
\item \textbf{Frequency Regression} Given the log of a token's frequency at each slice in the current window, take the slope of the best-fitting line.
\item \textbf{Message Frequency Regression} Given the log of the number of tweets in which a token appears for each slice in the current window, take the slope of the best-fitting line.
\item \textbf{User Frequency Regression} Given the log of the number of users using a token at each slice in the current window, take the slope of the best-fitting line.
\item \textbf{Average Frequency Difference} The difference between the token's frequency in the most recent slice and the average frequency across the current window.
\item \textbf{Message Average Frequency Difference} The difference between the number of messages in which a token appears in the most recent slice and the average number of messages containing that token across the current window.
\item \textbf{User Average Frequency Difference} The difference between the number of users who use a token in the most recent slice and the average number of users across the current window.
\item \textbf{Inter-Arrival Time} The average number of seconds between token occurrences in the given window.
\item \textbf{Entropy} The entropy of the set of tweets containing a given token.
\item \textbf{TF-IDF} The term frequency, inverse document frequency for a each token.
\item \textbf{TF-PDF} A modified version of TF-IDF called term frequency, proportional document frequency \cite{Bun:2002:TEN:645962.674082}.
\item \textbf{BursT} Weight using a combination of a given token's actual frequency and expected token frequency \cite{Lee:2011:BDT:2009463.2009531}.
\end{itemize}

We normalize each window's feature vectors into the range $[0, 1]$ to avoid biases from scale during classification by taking the maximum and minimum values for each feature in the current window.

\subsubsection{Training the Ensemble Classifier}

Discriminating between bursty tokens and stop words necessitates the use of a classification algorithm, and many different such algorithms exist.
In particular, the Scikit-learn Python package provides implementations for SVMs and RFs as well as an implementation of the ensemble classifier AdaBoost \cite{scikit-learn}.
Both SVMs and RFs have tunable parameters to select before integrating into AdaBoost, however, so we employed a grid search strategy to select parameters based on the F1 scores on our training and testing data.

For SVMs, we first had to decide whether to use a traditional linear model or employ a kernel.
Initial experiments showed linear SVMs performed quite poorly, and after using principal component analysis to reduce the dimension and looking at the labeled data, it fit well in a sphere rather than a clear linear plane. 
As a result, we use the radial basis kernel, which has two parameters: cost $c$ and kernel coefficient $\gamma$.
In searching the space of $c$ and $\gamma$, the grid covers powers of two such that $c = 2^x$, $x \in [-2, 10]$ and $\gamma = 2^y$, $y \in [-2, 5]$.
For each pair of parameter values, we train thirty different classifiers using repeated random subsampling, take the average F1 score, and select the parameter set with the highest F1 score.
Selecting parameter values for RFs is similar for the number of estimators $n$ and feature count $c'$ such that $n = 2^x$, $x \in [0, 10]$ and $c' = 2^y$, $y \in [1, 11]$.
This training procedure yielded the results shown in Table \ref{tab:scores}.

\begin{table}[htdp]
\caption{Classifier Parameter Scores}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Classifier} & \textbf{Params} & \textbf{F1-Score} \\ \hline
SVM & $c=64,$ & 0.588410 \\ 
& $\gamma=4$ & \\ \hline
RF & trees = 128, & 0.575301 \\
& features = 9 &  \\ \hline
\end{tabular}
\end{center}
\label{tab:scores}
\end{table}

These two classifiers are then combined using the Scikit-learn's AdaBoost implementation with four estimators.
We apply the resulting classifier to all training data and expand our set of known bursty tokens with those tokens that had a greater than 90\% likelihood of being part of the bursty class in one round of self-training (Scikit's AdaBoost implementation provides label likelihoods).
Regarding sliding window and slice size, preliminary investigations suggest a window size of ten minutes with a slice size of three minutes (each slice overlapped the next by two minutes) exhibited acceptable results.

\section{Experimental Results}
\label{sect:results}

To restate, the research question posed in this work is to determine whether a language-agnostic scheme for detecting moments of high interest can perform as well as a domain-specific frequency-based method in detecting events in sporting competitions.
We answer this question across three separate sporting events: the final two games of the 2013 MLB World Series, the 2014 NFL Super Bowl, and the final two matches of the 2014 FIFA World Cup.
These events were completely new data sets for both the baseline and LABurst and were not included in training, so any events detected by either algorithm were previously unseen events.

For each sporting competition, we generated ROC curves by varying the threshold parameter and calculating the true and false positive rates on detection corresponding to the indicator function $\mathbbm{1}_E(t)$ described above.
That is, the true positive rate equals the number of minutes in which an event both was detected \emph{and} occurred versus the number of minutes in which an event actually occurs.
Similarly, the false positive rate is the number of minutes in which the algorithm detects an event when no event actually occurred versus the total number of minutes in which no events occurred.

Prior to presenting comprehensive results, we first present performance curves for each event type.
For the 2013 World Series, LABurst and the baseline techniques exhibited similar performance, with a difference of only 0.02 (the baseline with 0.76 and the language-agnostic bursty method with 0.74).
Figure \ref{fig:roc2013WorldSeries} shows the how these two curves compare graphically, and we can see that neither curve completely dominates the other, and both perform better than random guessing.
In the Super Bowl, the difference between the two mechanisms (shown in Figure \ref{fig:roc2014SuperBowl}) is more pronounced with a difference in AUC near 0.1, with the baseline performing better.
LABurst exhibited a much higher false-positive rate in comparison to the baseline, which may be explained later in \ref{sect:eventDiscovery}.
Unlike the World Series and Super Bowl, however, the difference between the baseline and LABurst shows our LABurst method actually outperforms the baseline here with a difference in AUC of approximately 0.05.

\begin{figure*}
\centering
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=2in]{./figures/roc_2013_WorldSeries.png}
\caption{2013 World Series}
\label{fig:roc2013WorldSeries}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=2in]{./figures/roc_2014_SuperBowl.png}
\caption{2014 Super Bowl}
\label{fig:roc2014SuperBowl}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\includegraphics[width=2in]{./figures/roc_2014_WorldCup.png}
\caption{2014 World Cup}
\label{fig:roc2014WorldCup}
\end{subfigure}
\caption{Per-Sport ROC Curve Performance}
\label{fig:joinedPerf}
\end{figure*}

\subsection{Composite Results}

To compare comprehensive performance, we look to Figure \ref{fig:rocComprehensive}, which shows ROC curves for both methods across all three event types.
From this figure, we see the two methods perform nearly identically with AUC values of 0.7187 for the baseline and 0.7052 for our language-agnostic technique.
Assuming equal cost for false positives and false negatives and optimizing for the largest difference between true positive rate (TPR) and false positive rate (FPR), the baseline method shows a TPR of 0.5581 and FPR of 0.1408 with a difference of 0.4174 at a threshold value of 13.2.
Our language-agnostic method, on the other hand, has a TPR of 0.7105 and FPR of 0.3518 with a difference of 0.3587 at a threshold value of 2.
From these values, we see our approach achieves a higher true positive rate but at a cost of a higher false positive rate as a result.

\begin{figure}[hbtp]
\begin{center}
\includegraphics[width=3.25in]{./figures/roc_Comp.png}
\caption{Composite ROC Curves}
\label{fig:rocComprehensive}
\end{center}
\end{figure}

\subsection{Earthquake Detection}

Detecting interesting moments within sporting competitions as described above is a useful task for areas like advertising or automated highlight generation, but a more compelling and worthwhile task would be to detect higher-impact events like natural disasters.
The typical frequency-based approach is difficult here as it is impossible to know what events are about to happen where, and a list of target keywords to detect all such events would be long, leading to false positives.
Our method could be highly beneficial here as one need not know the target language or other pre-specified information.
Since Sakaki showed the feasibility of detecting earthquakes using Twitter, we pulled Twitter data for two earthquakes in Japan: a 7.1-magnitude quake off the coast of Honshu on 25 October 2013, and a 6.5-magnitude quake off the coast of Iwaki on 11 July 2014.

To determine whether our approach could detect these earthquake events, we applied the classifier trained and tested for the sporting domain to these Twitter sets and tracked the frequency of the term ``earthquake'' simultaneously.
Figures \ref{fig:2013Japan} and \ref{fig:2014Japan} show the frequencies for both methods for the 2013 and 2014 earthquakes respectively; the red dots indicate the earthquake times as reported by the United States Geological Survey (USGS).
From these figures, one can see the token ``earthquake'' sees a significant increase in usage when the earthquake occurs, and our language-agnostic method experiences a similar increase at the same moment for both events.
That is, both techniques identify the earthquake simultaneously.

\begin{figure*}
\centering
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=2.5in]{./figures/2013-japan-quake.png}
\caption{Honshu, Japan Earthquake - 25 October 2013}
\label{fig:2013Japan}
\end{subfigure}
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=2.5in]{./figures/2014-japan-quake.png}
\caption{Iwaki, Japan Earthquake - 11 July 2014}
\label{fig:2014Japan}
\end{subfigure}
\caption{Japanese Earthquake Detection}
\label{fig:joinedEarthquake}
\end{figure*}

Given our method's success here, one can now ask what tokens we identified as bursting when the earthquakes occurred.
Many of the tokens are in Japanese, and tokens at the peak of the earthquake events are shown in Table \ref{tab:japanTokens}.
We also extracted several tweets that contain the highest number of these tokens for the given time period, a selection of which include, \begin{CJK}{UTF8}{\myfont}``地震だあああああああああああああああああああああ,'' ``今回はチート使ってないから地震わからなかった,'' and ``地震だー.'' \end{CJK}
Google Translate\footnote{http://translate.google.com} translates these tweets as ``Ah ah ah ah ah ah ah ah ah Aa's earthquake,'' ``I did not know earthquake because not using cheat this time,'' and ``Over's earthquake'' respectively.

\begin{table}[htdp]
\caption{Tokens Classified as Busting During Events}
\begin{center}
\begin{tabular}{|p{1.45in} | p{1.45in} |}
\hline
\multicolumn{1}{|c|}{\textbf{Match}} & \multicolumn{1}{|c|}{\textbf{Bursty Tokens}} \\ \hline
Honshu, Japan -- 25 October 2013 & \c{c}dostum, \begin{CJK}{UTF8}{\myfont} 丈, 地, 夫, 怖, 波, 注, 津, 源, 福, 震 \end{CJK} \\ \hline
Iwaki, Japan -- 11 July 2014 & antojo, comida, sammy, \begin{CJK}{UTF8}{\myfont} び, ゆ, ビビ, 地, 怖, 急, 福, 警, 速, 震 \end{CJK} \\ \hline
\end{tabular}
\end{center}
\label{tab:japanTokens}
\end{table}

\section{Analysis}
\label{sect:analysis}

In comparing the baseline and language-agnostic techniques, it is important to understand the baseline provides little in the way of discovering previously unknown tokens or significant events that do not conform to a priori knowledge.
Our language-agnostic method's real advantage is that it not only directly supports this notion of discovery but also provides direct insight into these moments of high interest by tagging relevant bursting tokens.

\subsection{Identifying Event-Related Tokens}

As mentioned, where the baseline requires the user to specify interesting or event-related tokens prior to any data processing or analysis, our approach identifies these event-related tokens automatically.
These tokens may include misspellings, colloquialisms, and cross language boundaries, which makes them hard to know before hand.
The 2014 World Cup presents an interesting case for finding these otherwise unexpected tokens because the event has enormous international viewership; as such, many Twitter users of many different languages are likely tweeting about the same event.

To explore the tokens generated during these high-profile events, we look to those tokens identified as bursting during several events in the final two World Cup matches.
Table \ref{tab:burstyTokens} shows a selection of events from these matches and a subset of those tokens classified as bursting during the events (one should note the list is not exhaustive owing to formatting and space constraints).

\begin{table}[htdp]
\caption{Tokens Classified as Busting During Events}
\begin{center}
\begin{tabular}{|p{0.75in}|p{0.7in}| p{1.45in} |}
\hline
\multicolumn{1}{|c|}{\textbf{Match}} & \multicolumn{1}{|c|}{\textbf{Event}} & \multicolumn{1}{|c|}{\textbf{Bursty Tokens}} \\ \hline
Brazil v. Netherlands, 12 July 2014 & Netherlands' Van Persie scores a goal on a penalty at 3', 1-0 & 0-1, 1-0, 1:0, 1x0, card, goaaaaaaal, goal, gol, goool, holandaaaa, k\i{}rm\i{}z\i{}, pen, penal, penalti, p\^{e}nalti, persie, red \\ \hline
Brazil v. Netherlands, 12 July 2014 & Brazil's Oscar get's a yellow card at 68' & dive, juiz, penalty, ref \\ \hline
Germany v. Argentina, 13 July 2014 & Germany's G\"{o}tze scores a goal at 113', 1-0 & goaaaaallllllll, goalllll, godammit, goetze, gollllll, gooooool, gotze, gotzeeee, g\"{o}tze, nooo, yessss, \begin{CJK}{UTF8}{\myfont} ドイツ\end{CJK} \\ \hline
\end{tabular}
\end{center}
\label{tab:burstyTokens}
\end{table}

Several interesting artifacts emerge from this table, first of which is that one can get an immediate sense of the detected event from tokens our algorithm presents. 
For instance, the prevalence of the token ``goal'' and its variations clearly indicate a team scored in the first and third events in Table \ref{tab:burstyTokens}; similarly, bursting tokens associated with the middle event regarding Oscar's yellow card reflect his penalty for diving.
Beyond the pseudo event description put forth by the identified tokens, this reference to diving and to specific player and teams names in the first and third events are also of significant interest.
In the first event, one can infer that the Netherlands scored since ``holandaaaa'' is flagged along with ``persie'' from the Netherlands' player Van Persie, and likewise for Germany's G\"{o}tze in the third event (and the accompanying variations of his name).
These terms would be difficult to capture beforehand as would be required in the baseline and would likely not be related to every event or every type of sporting event.

Finally, the last artifact of note is that the set of bursty tokens displayed includes tokens from several different languages: English for ``goal'' and ``penalty,'' Spanish for ``gol'' and ``penal,'' Brazilian Portuguese for ``juiz'' (meaning ``referee''), as well as the Arabic for ``goal'' and Japanese for ``Germany.''
Since these words are semantically similar but syntactically dissimilar, typical normalization schemes could not capture these connections.
Instead, capturing these words in the baseline would require a pre-specific keyword list in all possible languages or the inclusion of an expensive machine translation system that was also capable of normalizing within different languages (to collapse ``goool'' down to ``gol'' for example).

\subsection{Discovering Unanticipated Moments}
\label{sect:eventDiscovery}

One particular weakness present in the baseline is that it is unable to capture unexpected events or events that do not conform to the keyword list.
This deficiency means analysts might miss significant events within these competitions, especially if they are not directly related to scores or penalties, such as Uruguay's Luis Suarez's biting the Italy's Giorgio Chiellini during a World Cup match on 24 June since no foul was called at the time.
Other instances of particularly dramatic play or events that happen at the larger sporting event but not necessarily on the field might be missed as well.

We can see instances of such omissions in the last game of World Cup.
Figure \ref{fig:worldCupFreqs} shows the frequencies for target tokens for the baseline in blue and bursty tokens for our method in green.
From this graph, we can see the first, obvious incidence in Peak \#1 where our bursty method exhibits a peak that is missed by the baseline in the first few points of data.
The primary tokens appearing in this peak are ``puyol,'' ``gisele,'' and ``bundchen,'' which correspond to former Spanish player Carles Puyol and model Gisele Bundchen, who presented the World Cup trophy prior to the match.
At peak \#2, slightly more than eighty minutes into the data (which is sixty minutes into the match), our burst analysis sees another peak that is otherwise relatively minor in the baseline graph.
Upon further exploration, tokens present in this peak refer to Argentina's substituting in Ag\"{u}ero for Lavezzi at the beginning of the match's second half.

\begin{figure}[hbtp]
\begin{center}
\includegraphics[width=2.5in]{./figures/wc0713freq-labeled.png}
\caption{Baseline and LA Bursty Frequencies}
\label{fig:worldCupFreqs}
\end{center}
\end{figure}

Given our detection of these moments of interest that do not related to score or penalty events, it is perhaps unsurprising that our burst detection technique exhibits a higher false-positive rate compared to the baseline.
Since our approach is both language- and domain-agnostic, it makes sense that it would detect additional events beyond the game start/end, score, and penalty events our data counts as ground truth.
A better comparison between the accuracies of the two approaches may be to identify only those peaks in which the keywords used in the baseline are identified as bursty in our method, but such a test disregards our approach's additional power.

\subsection{Real-Time Usage and Event Persistence}

In comparing these two event detection methods, we must also address their abilities to handle streaming data and the lag between an event and its detection by either of these mechanisms.
The baseline technique processes data minute by minute and therefore has at most a minute of lag between input and discovery.
Our technique, on the other hand, exhibits a lag of at most the slice size, which in the case of this paper is three minutes.

Another important aspect to consider is the length of time in which an event's peak persists.
For the baseline, event detection achieves its highest accuracy when events are flagged in the minute they occur and the minute immediately following.
Our approach logically follows the slice length such that events persist for approximately three minutes.

\section{Future Work}
\label{sect:future}

% Real streaming context, Different classifiers, different levels of granularity, more domain adaptation research, temporal topic clustering, Facebook?
While the experiments outlined herein establish the utility of language-agnostic event detection, several avenues of research can follow up on this foundation.
First, though this work is applied in the streaming context, little effort was made to enforce near-real-time computation constraints; with the growing popularity of stream-centric processing frameworks like Apache Storm (as used by Petrovi\'{c} et al.) or Apache Spark Streaming, one has considerable latitude in exploring ways to enforce such constraints.
Secondly, our selection of classifiers used in our ensemble was based primarily on ease of use given our features; deep learning systems are more complex but could provide enhanced capabilities in detecting bursting patterns in social media streams.
Thirdly, we specifically targeted moments of high interest, which often correspond to instantaneous events, but it is possible that these same techniques could be applied at different levels of granularity (per hour or per day for instance) to detect events of larger scales; one could then apply Kleinberg's notion of event hierarchies across multiple temporal granularities and track different aspects of the same event.
Additionally, an implicit assumption made in this work is that tokens that experience bursts at the same time are related in some way, which allows us to detect events using token frequency; this assumption has interesting consequences with regarding to language-agnostic topic detection.
That is, one could cluster tokens with similar temporal signatures to identify topics across languages, which would otherwise be impossible if one were to rely solely on semantic similarity or would require applying machine translation to reduce all text in the stream to the same language.
Finally, though this paper exclusively leverages data from Twitter's public stream, our techniques should be applicable to streams from other social networks as well, and how events burst on more media-centered networks like Flickr or Pinterest might reveal interesting photographic representations of an event.

\section{Conclusions}
\label{sect:conlusions}

To revisit our motivations, the goal for this experiment was to demonstrate the feasibility of detecting highly compelling or interesting moments through analyzing temporal characteristics from unfiltered Twitter data streams.
While many social media-based event detection systems require some form of prespecified seed list and/or language model processing, our approach is more flexible, lighter weight, and easily adaptable to different domains.
Our results show that by leveraging temporal characteristics to identify bursty tokens and using frequency of these bursty tokens, we can detect significant events across a collection of disparate sporting competitions with a level of performance nearly equivalent to an existing, domain-specific baseline.

Similar performance to the baseline is only part of the story, however, as our approach offers notable flexibility in identifying bursting tokens without normalization and across language boundaries.
With this versatility also comes support for event description since we no longer rely on predetermined keywords; that is, we can get a sense of the occurring event by inspecting the bursty tokens.
Finally, these advantages culminate in powerful tool for event \emph{discovery} in that it can unanticipated instances of high interest that we did not expect, regardless of the source language, which makes this technique particularly useful for journalists and newswire sources who have a need to know about events on the ground, as they happen but cannot know a priori what the event may be about in all cases.

%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
This work made use of the Open Science Data Cloud (OSDC), which is an Open Cloud Consortium (OCC)-sponsored project. 
The OSDC is supported in part by grants from Gordon and Betty Moore Foundation and the National Science Foundation and major contributions from OCC members like the University of Chicago. 

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sources}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\end{document}
